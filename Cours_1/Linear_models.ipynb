{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MorganGautherot/Machine_Learning_Courses/blob/master/Lecture_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jhv5lEVgr7Ba"
   },
   "source": [
    "# Linear models\n",
    "*Author -- Morgan Gautherot*\n",
    "\n",
    "If you have any question please feel free to send me an email at :\n",
    "\n",
    "gautherotmorgan0@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VXMahy51cmvi"
   },
   "source": [
    "## 1. Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lUZ7TrAfcqS2"
   },
   "source": [
    "In this exercice, you will implement linear regression and get to see it work on data. Before starting on this programming exercice, we strongly recommend to pay attention to the last part which deal with python for data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KxMHgSt5dMA_"
   },
   "source": [
    "### 1.1 Univariate linear regression\n",
    "\n",
    "You will start with the implementation of linear regression with one variable. The goal is to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for openning a new outlet. The chain already has trucks in various cities and you have data for profits and populations from cities. You would like to use this data to help you select which city to expand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJX3TT6JgbqX"
   },
   "source": [
    "First step you will run this cell to import all the package and initialize the seed you need for the rest of this exercice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UBn5MbcSe2K-"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V80y20Zcg9UC"
   },
   "source": [
    "To work we need data, run this piece of code and upload the dataset : univariate_regression.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "swX1gjGdeQtg"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k3eCGWeueoWh"
   },
   "outputs": [],
   "source": [
    "univariate_regression = np.genfromtxt('univariate_regression.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y0aNFfr-1wfi"
   },
   "outputs": [],
   "source": [
    "# We construct the X dataset \n",
    "X = univariate_regression[:, 0]\n",
    "\n",
    "# We construct the Y dataset\n",
    "Y = univariate_regression[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sbuRIKZLhbGs"
   },
   "source": [
    "You can use matplotlib to plot the data and see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lRvEwTTefBsF"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X, Y)\n",
    "plt.title('Scatter plot of training data', size='xx-large')\n",
    "plt.xlabel('Population of City in 10.000s')\n",
    "plt.ylabel('Profit in $10.000s')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CxySmyr7lkxk"
   },
   "source": [
    "Now we can start to code our linear regression. Recall the hypothesis for univariate regression :\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B6HIqEaxmamP"
   },
   "source": [
    "$$h_{w}(x) =w_0 + w_1 x_1$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nFgmiP7znjbl"
   },
   "source": [
    "Start by complete this function which will compute the above equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aVkCEM9Znrhf"
   },
   "outputs": [],
   "source": [
    "def hypothesis(X, W):\n",
    "  \"\"\" Predict our h(x) in function of their features \n",
    "  INPUT :\n",
    "    X float\n",
    "    W shape (n,)\n",
    "  OUTPUT :\n",
    "    h(X) float\"\"\"\n",
    "  \n",
    "### Your code start here ###\n",
    "\n",
    "### Your code end here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m0nQjU_nqxs6"
   },
   "outputs": [],
   "source": [
    "W = np.array((1, 4))\n",
    "print(\"X = \" + str(X[1]) + \"\\nw_0 = 1\\nw_1 = 4\\nh(X) = \" + str(hypothesis(X[1], W)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-GMWdF11qzvR"
   },
   "source": [
    "You should have :\n",
    "\n",
    "X = 5.5277\n",
    "\n",
    "w_0 = 1\n",
    "\n",
    "w_1 = 4\n",
    "\n",
    "h(X) = 23.1108"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oMrPpF8WuqJH"
   },
   "source": [
    "Now you can predict, you have to compute the error of each prediction. For that you have to implement the cost function:\n",
    "$$ J(W)=\\frac{1}{2m} \\sum_{i=1}^{m} (h_w(x^{(i)})-y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFPG_dYrvo8y"
   },
   "source": [
    "Complete this function below with the above formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F_ybOJQcvvaW"
   },
   "outputs": [],
   "source": [
    "def ComputeCost(X, Y, W):\n",
    "  \"\"\" Compute cost for linear regression \n",
    "  INTPUT :\n",
    "    X shape(m,)\n",
    "    Y shape(m,)\n",
    "    W shape(n,)\n",
    "  OUTPUT :\n",
    "    J float\"\"\"\n",
    "\n",
    "  m = len(Y)\n",
    "\n",
    "  J = 0\n",
    "\n",
    "  ### Your code start here ###\n",
    " \n",
    "  ### Your code end here ###\n",
    "\n",
    "  return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SASE77h2qvNX"
   },
   "outputs": [],
   "source": [
    "W = (1, 1)\n",
    "print(\"With W = (1, 1)\\nCost Function = \" + str(ComputeCost(X, Y, W)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0Ku06ZZ3gqS"
   },
   "source": [
    "You should have this result :\n",
    "\n",
    "With W = (1, 1)\n",
    "\n",
    "Cost Function = 10.266520491383503"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pWmAI1gI4fNP"
   },
   "source": [
    "You have the two main pieces of our linear regression. We can predict new output values and compute the error. We now have to implement the gradient descent algorithm to minimize the error and get the best model.  \n",
    "\n",
    "$$ w_j := w_j  - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_w(x^{(i)})-y^{(i)})x_j^{(i)}$$ Simultaneously update $w_j$ for all j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z7vOxc4K6CQN"
   },
   "outputs": [],
   "source": [
    "def gradientDescent(X, Y, W, alpha, num_iters):\n",
    "  \"\"\" GradientDescent compute gradient descent to learn best W.\n",
    "  To track the improvment of our cost we compute it at each iteration.\n",
    "  INPUT :\n",
    "    X          shape(m,)\n",
    "    Y          shape(m,)\n",
    "    W          shape(n,)\n",
    "    alpha      float\n",
    "    num_iters  integer\n",
    "  OUPUT :\n",
    "    W          shape(n,)\n",
    "    J_history  shape(num_iters,)\"\"\"\n",
    "  \n",
    "  m = len(Y)\n",
    "  J_history = np.zeros(num_iters)\n",
    "  \n",
    "  for iters in range(0, num_iters):\n",
    "    ### Your code start here ###\n",
    "\n",
    "    ### Your code end here ###\n",
    "    J_history[iters] = ComputeCost(X, Y, W)\n",
    "    \n",
    "  return W, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-9Mix9Vz8WE"
   },
   "outputs": [],
   "source": [
    "W = [0, 0]\n",
    "alpha = 0.01\n",
    "iterations = 2000\n",
    "W, J_history = gradientDescent(X, Y, W, alpha, iterations)\n",
    "\n",
    "print(\"Values of the parameters W after 2000 iterations of gradient descent :\\n\"\n",
    "      + str(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHrMET8CBkNY"
   },
   "source": [
    "Expected values :\n",
    "\n",
    "[-3.788, 1.182]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UuiutTk9KSad"
   },
   "outputs": [],
   "source": [
    "plt.plot(J_history)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"J(W)\")\n",
    "plt.title(\"Evolution of the cost after each iterations of gradient descent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PpoIkrWcJEd1"
   },
   "source": [
    "You have all the pieces to implement the trainning of our model. The training is the action to determine the weight in function of our example and find the global minima. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XVwzsCNiJVQ0"
   },
   "outputs": [],
   "source": [
    "def lr_fit(X, Y, W, alpha, num_iters) :\n",
    "  \"\"\" lr_fit train our algorithm to find the best W for our prediction\n",
    "  INPUT :\n",
    "    X          shape(m,)\n",
    "    Y          shape(m,)\n",
    "    W          shape(n,)\n",
    "    alpha      float\n",
    "    num_iters  integer\n",
    "  OUPUT :\n",
    "    W          shape(n,)\"\"\"\n",
    "  \n",
    "  \n",
    "  W, _ = gradientDescent(X, Y, W, alpha, num_iters)\n",
    "  \n",
    "  return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWeS3-ADLGn5"
   },
   "outputs": [],
   "source": [
    "W = [0, 0]\n",
    "alpha = 0.01\n",
    "iterations = 2000\n",
    "W = lr_fit(X, Y, W, alpha, iterations)\n",
    "\n",
    "print(\"Values of the parameters W after 2000 iterations of gradient descent :\\n\"\n",
    "      + str(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p2aMd3R1MqRq"
   },
   "source": [
    "Expected values :\n",
    "\n",
    "[-3.788, 1.182]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4oAjtw_RMtQ_"
   },
   "source": [
    "You already implement a function which with a x and W give your the prediction, now you will implement a function which given a vector X and W all the prediction for each one of the X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xN26cI-lNdZA"
   },
   "outputs": [],
   "source": [
    "def lr_predict(X, W) :\n",
    "\"\"\" lr_predict make prediction for each observations of X\n",
    "  INPUT :\n",
    "    X          shape(m,)\n",
    "    W          shape(n,)\n",
    "  OUPUT :\n",
    "    h          shape(n,)\"\"\"\n",
    "  m = len(X)\n",
    "  \n",
    "  h = np.zeros(m)\n",
    "\n",
    "  for i in range(m) :\n",
    "    h[i] = hypothesis(X[i], W)\n",
    "               \n",
    "  return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQIsVJz3BF9W"
   },
   "source": [
    "You can see your linear boundary in red on the plot below. It's with this function you use to predict new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T-azJXIvBTfl"
   },
   "outputs": [],
   "source": [
    "W = [0, 0]\n",
    "alpha = 0.01\n",
    "iterations = 2000\n",
    "\n",
    "W = lr_fit(X, Y, W, alpha, iterations)\n",
    "\n",
    "x_lr = list(np.arange(4.5, 24))\n",
    "y_lr = lr_predict(x_lr, W)  \n",
    "plt.plot(x_lr, y_lr, c='red')\n",
    "\n",
    "plt.scatter(X, Y)\n",
    "\n",
    "plt.title('Scatter plot of training data', size='xx-large')\n",
    "plt.xlabel('Population of City in 10.000s')\n",
    "plt.ylabel('Profit in $10.000s')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRMWO2_kC2qh"
   },
   "source": [
    "### 1.2 Multivariate linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07qn1iexFqWJ"
   },
   "source": [
    "For this exercice suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model  of houses prices.\n",
    "\n",
    "**!!!   In this part all you algorithm must be vectorize.   !!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LND_Oxe7hrIz"
   },
   "source": [
    "Let's import 'multivariate_regression.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VBx4uVQ9BE6w"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "THQ5lLltGLZ_"
   },
   "source": [
    "In this dataset, the first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "etVL61j9DMvt"
   },
   "outputs": [],
   "source": [
    "# Creation of the dataset\n",
    "multivariate_regression = np.genfromtxt('multivariate_regression.txt', delimiter=',')\n",
    "\n",
    "m, _ = multivariate_regression.shape\n",
    "\n",
    "#X = np.stack(multivariate_regression[:, :2], axis=1)\n",
    "X = multivariate_regression[:, :2] \n",
    "Y = multivariate_regression[:, 2].reshape((1, m))\n",
    "\n",
    "# Standardisation of the features\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "X_1 = np.stack(np.hstack((np.ones(m).reshape(m, 1), X)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8NpByzfLHn6V"
   },
   "source": [
    "Like the last part we start by the hypothesis function. Remind you that our prediction looks like that :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GnWNPVjkHfcC"
   },
   "source": [
    "$$h(X) = W^TX$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-rvo-i0hXZ0j"
   },
   "source": [
    "The result is the same but it's more convenient to have dataframe with examples 'm' in rows and features 'n' in columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W14vnuK_G7h0"
   },
   "outputs": [],
   "source": [
    "def predict(X, W):\n",
    "  \"\"\" Predict our h(x) in function of their features \n",
    "  INPUT :\n",
    "    X shape(n, m)\n",
    "    W shape(n, 1)\n",
    "  OUTPUT :\n",
    "    h shape(1, m)\"\"\"\n",
    "### Your code start here ###\n",
    "\n",
    "### Your code end here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xPIp4eRoI1D9"
   },
   "outputs": [],
   "source": [
    "W = np.array((1, 1, 1)).reshape(3, 1)\n",
    "\n",
    "print(\"X = \" + str(X_1[:, 1]) + \"\\nW = \"+str(np.squeeze(W))+\"\\nh(X) = \" + str(predict(X_1[:, 1], W)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wkvYxTaI0xh"
   },
   "source": [
    "Expected values :\n",
    "\n",
    "X = [ 1. -0.5096407 -0.22609337]\n",
    "\n",
    "W = [1, 1, 1]\n",
    "\n",
    "h(X) = 0.26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xj_gqcQBHeQf"
   },
   "source": [
    "Now you can predict new observations, you have to compute the error of each prediction. For that, you have to implement the cost function. \n",
    "\n",
    "Remind you the equation of the cost function :\n",
    "$$ J(W)=\\frac{1}{2m} (h(X)-Y)(h(X)-Y)^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ruLHcgiRoIEr"
   },
   "outputs": [],
   "source": [
    "def ComputeCost(X, W, Y):\n",
    "  \"\"\" Compute cost for linear regression \n",
    "  INPUT :\n",
    "    X shape(n, m)\n",
    "    W shape(n, 1)\n",
    "    Y shape(1, m)\n",
    "  OUTPUT :\n",
    "    J float\"\"\"\n",
    "\n",
    "  _, m = X.shape\n",
    "\n",
    "  J = 0\n",
    "\n",
    "### Your code start here ###\n",
    "\n",
    "### Your code end here ###\n",
    "\n",
    "  return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZIq0JuK1o08n"
   },
   "outputs": [],
   "source": [
    "W = np.array((1,1,1)).reshape(3, 1)\n",
    "\n",
    "print(\"With W = \"+str(np.squeeze(W))+\"\\nCost Function = \" + str(np.squeeze(ComputeCost(X_1, W, Y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bl8-gbsDo41J"
   },
   "source": [
    "You should have this result :\n",
    "\n",
    "With W = [1, 1, 1]\n",
    "\n",
    "Cost Function = 65591047222.90259"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1fbcsFJuwLBH"
   },
   "source": [
    "You have the two main pieces of our linear regression. We can predict new output values and compute the error. We know have to implement the gradient descent algorithm to minimize the error and get the best model.  \n",
    "\n",
    "Remind you the algorithm of gradient descent :\n",
    "\n",
    "$$ W := W - \\frac{\\alpha}{m}X .(h(X)-Y)^T $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qLFht0Lbttca"
   },
   "outputs": [],
   "source": [
    "def gradientDescent(X, W, Y, alpha, num_iters):\n",
    "  \"\"\" gradientDescent compute gradient descent to learn best W\n",
    "  INPUT :\n",
    "    X           shape(n, m)\n",
    "    W           shape(n, 1)\n",
    "    Y           shape(1, m)\n",
    "    alpha       float\n",
    "    num_iters   integer\n",
    "  OUTPUT :\n",
    "    W           shape(n, 1)\n",
    "    J_history   shape(num_iters,)\"\"\"\n",
    "  \n",
    "  _, m = X.shape\n",
    "  J_history = np.zeros(num_iters)\n",
    "  \n",
    "  for i in range(0, num_iters):\n",
    "    \n",
    "    ### Your code start here ###\n",
    "\n",
    "    ### Your code end here ###\n",
    "    \n",
    "    J_history[i] = ComputeCost(X, W, Y)\n",
    "    \n",
    "  return W, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0xJQM-eTOzg9"
   },
   "outputs": [],
   "source": [
    "W = np.array((0, 0, 0)).reshape((3,1))\n",
    "alpha = 0.01\n",
    "iterations = 200\n",
    "W, J_history = gradientDescent(X_1, W, Y, alpha, iterations)\n",
    "print(\"Values of the parameters W after \" + str(iterations) +  \\\n",
    "      \" iterations of gradient descent :\\n\" + str(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Vgnvh9iPDr3"
   },
   "source": [
    "Expected values : \n",
    "\n",
    "[[294804.28212715]\n",
    "\n",
    "[ 83217.03697925]\n",
    "\n",
    "[ 15220.03137818]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kqot20wiSGh4"
   },
   "source": [
    "You can visualize your cost function minimization with this graph. It can be helpful to see if you have a correct learning rate or if you have enough iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ckn-hJhSNIar"
   },
   "outputs": [],
   "source": [
    "plt.plot(J_history)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"J(W)\")\n",
    "plt.title(\"Minimization of the cost function after each iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XgkAz9TYSD3f"
   },
   "source": [
    "You have all the pieces to implement the trainning of our model. The training is the action to determine the weight in function of our example and find the global minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hm3ZBUdER-4S"
   },
   "outputs": [],
   "source": [
    "def lr_fit(X, W, Y, alpha, num_iters) :\n",
    "  \"\"\" lr_fit train our algorithm to find the best W for our prediction\n",
    "  INPUT :\n",
    "    X          shape(m, n-1)\n",
    "    W          shape(n, 1)\n",
    "    Y          shape(1, m)\n",
    "    alpha      float\n",
    "    num_iters  integer\n",
    "  OUPUT :\n",
    "    W          shape(n, 1)\n",
    "    J_history   shape(num_iters,)\"\"\"\n",
    "  \n",
    "  m, _ = X.shape\n",
    "  \n",
    "  X_1 = np.stack(np.hstack((np.ones(m).reshape(m, 1), X)), axis=1)\n",
    "  \n",
    "  W, J_history = gradientDescent(X_1, W, Y, alpha, num_iters)\n",
    "  \n",
    "  return W, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3fDcL35iU95u"
   },
   "outputs": [],
   "source": [
    "W = np.array([0, 0, 0]).reshape(3, 1)\n",
    "alpha = 0.01\n",
    "iterations = 200\n",
    "W, _ = lr_fit(X, W, Y, alpha, iterations)\n",
    "\n",
    "print(\"Values of the parameters W after 2000 iterations of gradient descent :\\n\"\n",
    "      + str(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awgYR26DVal-"
   },
   "source": [
    "Expected values : \n",
    "\n",
    "[[294804.28212715]\n",
    "\n",
    "[ 83217.03697925]\n",
    "\n",
    "[ 15220.03137818]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "406sSKsTys2B"
   },
   "outputs": [],
   "source": [
    "iterations = 200\n",
    "\n",
    "alpha = [0.3, 0.1, 0.003, 0.001]\n",
    "\n",
    "J_history = np.zeros((iterations, len(alpha)))\n",
    "\n",
    "for i in range(0, len(alpha)) :\n",
    "  \n",
    "  W = np.array((0, 0, 0)).reshape((3,1))\n",
    "\n",
    "  W, J_history[:, i] = lr_fit(X, W, Y, alpha[i], iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sFas-xtGBtE6"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot(J_history[:, 0], label=\"alpha = 0.3\")\n",
    "plt.plot(J_history[:, 1], label=\"alpha = 0.1\")\n",
    "plt.plot(J_history[:, 2], label=\"alpha = 0.03\")\n",
    "plt.plot(J_history[:, 3], label=\"alpha = 0.01\")\n",
    "\n",
    "plt.title('Training with different learning rate', size='xx-large')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"J(W)\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIcc157dVvLB"
   },
   "source": [
    "Like I said before this kind of graphic can help you to take a great learning rate and a great number of iterations. When alpha equal 0.01 you can see that our cost function has not yet reach it's minimum, you can increase the learning rate to get quicker the best parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WuxPajkyWdTI"
   },
   "source": [
    "## 2. Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ftvWsdPWeqh"
   },
   "source": [
    "In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university. \n",
    "\n",
    "Suppose that you are the administrator of a university department and you want to determine each applicant’s chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for logistic regression. For each training example, you have the applicant’s scores on two exams and the admissions decision. \n",
    "\n",
    "Your mission if you choose to accept it, is to build a classiﬁcation model that estimates an applicant’s probability of admission based the scores from those two exams.\n",
    "\n",
    "**!!! Be careful, all implementations must use vectorization !!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f2f-paM3hwJe"
   },
   "source": [
    "Let's import 'logistic_regression.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "faA19lFmv21a"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4t_JIFhAx6Ik"
   },
   "outputs": [],
   "source": [
    "logistic_regression = np.genfromtxt('logistic_regression.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lX3ch8CLNuIM"
   },
   "source": [
    "Before starting to implement any learning algorithm, it is always good to visualize the data if possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M9tA4TR8vX82"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.scatter(logistic_regression[logistic_regression[:, 2]==0, 0], logistic_regression[logistic_regression[:, 2]==0, 1], label='Admitted')\n",
    "plt.scatter(logistic_regression[logistic_regression[:, 2]==1, 0], logistic_regression[logistic_regression[:, 2]==1, 1], label='Not admitted', marker='v')\n",
    "\n",
    "plt.title('Scatter plot of training data', size='xx-large')\n",
    "plt.xlabel('Exam 1 score')\n",
    "plt.ylabel('Exam 2 score')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xwI9Oq9pOF5g"
   },
   "source": [
    "The data are plot where the axes are the two exam scores, and the positive and negative examples are shown with diﬀerent markers and colours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5gxz4O3Ay1OG"
   },
   "outputs": [],
   "source": [
    "m, n = logistic_regression[:, :2].shape\n",
    "X = logistic_regression[:, :2]\n",
    "X_1 = np.stack(np.hstack((np.ones(m).reshape(m, 1), X)), axis=1)\n",
    "Y = logistic_regression[:, 2].reshape(1, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3JguaSeXPc_n"
   },
   "source": [
    "You start by implement the sigmoid function :\n",
    "$$ g(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MdSfIclCPYqC"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "  \"\"\" Apply sigmoid function to a number\n",
    "  INPUT :\n",
    "    z shape(1, m)\n",
    "  OUTPUT :\n",
    "    g(z) shape(1, m)\"\"\"\n",
    "  \n",
    "  ### Your code start here ###\n",
    "\n",
    "  ### Your code start here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qwui1XXDTEP1"
   },
   "outputs": [],
   "source": [
    "x = np.array(range(-10, 10))\n",
    "y = sigmoid(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VQZb9549TNAt"
   },
   "source": [
    "If you have a good implementation you will see the same figure than the one in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xTgRglr1Q2RC"
   },
   "source": [
    "You now use your sigmoid function to construct your predict in a vectorize way.\n",
    "\n",
    "$$ h(X) = g(W^TX)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2l1e91EarHL"
   },
   "source": [
    "The result is the same but it's more convenient to have dataframe with examples 'm' in rows and features 'n' in columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zkehnEvNRi8M"
   },
   "outputs": [],
   "source": [
    "def predict(X, W):\n",
    "  \"\"\" This function makes predictions by linearly combining W and X. \n",
    "     Sigmoid is apply to born the output between [0, 1]\n",
    "     INPUT : \n",
    "        X shape(n, m)\n",
    "        W shape(n, 1)\n",
    "     OUTPUT :  \n",
    "        h shape(1, m)\"\"\"\n",
    "  \n",
    "  ### Your code start here ###\n",
    "\n",
    "  ### Your code start here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zhzBAIB_UqB3"
   },
   "outputs": [],
   "source": [
    "W = np.array((-24, 0.2, 0.2)).reshape(3, 1)\n",
    "\n",
    "h_5 = predict(X_1[:, :5], W)\n",
    "print(h_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbHQGURpbKOs"
   },
   "source": [
    "Expected values :\n",
    "\n",
    "[0.187, 0.000105, 0.0953, 0.995, 0.999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BO7mvsgxcJhI"
   },
   "source": [
    "Your are now ready to implement you cost function :\n",
    "\n",
    "$$ J(W)= -\\frac{1}{m}\\sum^{N}_{i=1}y_i \\log{(h(x^{(i)}))}+(1-y_i)\\log{(1-h(x^{(i)}))}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "owV755ndYZhY"
   },
   "outputs": [],
   "source": [
    "def computeCost(X, W, Y) :\n",
    "  \"\"\" Compute the cost for the W parameters\n",
    "  INPUT : \n",
    "    X shape(n, m)\n",
    "    W shape(n, 1)\n",
    "    Y shape(1, m)\n",
    "  OUTPUT : \n",
    "    J float\"\"\"\n",
    "  \n",
    "  ### Your code start here ###\n",
    "  \n",
    "  ### Your code start here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hBfWD9_9e13l"
   },
   "outputs": [],
   "source": [
    "W = np.array((-24, 0.2, 0.2)).reshape(3, 1)\n",
    "\n",
    "J = computeCost(X_1, W, Y)\n",
    "\n",
    "print(\"With W = \"+str(np.squeeze(W))+\"\\nCost Function = \" +str(np.squeeze(J)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aX0tmS53fv4G"
   },
   "source": [
    "Expected cost  :  0.218"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZXES5hv3xZyM"
   },
   "source": [
    "The goal now we can compute the cost is to minimize it with gradient descent :\n",
    "\n",
    "$$ W := W - \\frac{\\alpha}{m}X (h(X)-Y)^T $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3PaDK2QOzSGr"
   },
   "outputs": [],
   "source": [
    "def gradientDescent(X, W, Y, alpha, num_iters):\n",
    "  \"\"\" gradientDescent updates W by taking num_iters\n",
    "  gradient steps with learning rate alpha \n",
    "  INPUT :\n",
    "    X         shape(n, m)\n",
    "    W         shape(n, 1)\n",
    "    Y         shape(1, m)\n",
    "    alpha     float\n",
    "    num_iters integer\n",
    "  OUTPUT :\n",
    "    W         shape(n, 1)\n",
    "    J_history shape(num_iters,)\"\"\"\n",
    "  \n",
    "  _, m = X.shape\n",
    "  \n",
    "  J_history = np.zeros(num_iters)\n",
    "  \n",
    "  for i in range(0, num_iters):\n",
    "    \n",
    "    ### Your code start here ###\n",
    "\n",
    "    ### Your code end here ###\n",
    "    \n",
    "    J_history[i] = computeCost(X, W, Y)\n",
    "    \n",
    "  return W, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kYum5y5fzesL"
   },
   "outputs": [],
   "source": [
    "W = np.array((-24, 0.2, 0.2)).reshape((3,1))\n",
    "alpha = 0.001\n",
    "iterations = 20\n",
    "\n",
    "W, J_history = gradientDescent(X_1, W, Y, alpha, iterations)\n",
    "\n",
    "print(\"Values of the parameters W after \" + str(iterations) +  \\\n",
    "      \" iterations of gradient descent :\\n\" + str(W))\n",
    "print(\"Values of the cost function : \" +str(J_history[iterations-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xv1EQ_pq8eEt"
   },
   "source": [
    "Expected parameters values : \n",
    "\n",
    "[[-24.00\n",
    " [  0.20]\n",
    " [  0.19]]\n",
    " \n",
    " Expected cost values : \n",
    " \n",
    " 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K1Urouulz3KO"
   },
   "outputs": [],
   "source": [
    "plt.plot(J_history)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"J(W)\")\n",
    "plt.title(\"Minimization of the cost function after each iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vj_hig4-gDTI"
   },
   "source": [
    "You have all the pieces to implement the trainning of our model. The training is the action to determine the weight in function of our example and find the global minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WbOPClmtgJy3"
   },
   "outputs": [],
   "source": [
    "def lr_fit(X, W, Y, alpha, num_iters) :\n",
    "  \"\"\" lr_fit train our algorithm to find the best W for our prediction\n",
    "  INPUT :\n",
    "    X          shape(m, n-1)\n",
    "    W          shape(n, 1)\n",
    "    Y          shape(1, m)\n",
    "    alpha      float\n",
    "    num_iters  integer\n",
    "  OUPUT :\n",
    "    W          shape(n,)\n",
    "    J_history   shape(num_iters,)\"\"\"\n",
    "  \n",
    "  m, _ = X.shape\n",
    "  \n",
    "  X_1 = np.stack(np.hstack((np.ones(m).reshape(m, 1), X)), axis=1)\n",
    "  \n",
    "  W, J_history = gradientDescent(X_1, W, Y, alpha, num_iters)\n",
    "  \n",
    "  return W, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8I9ivfCTgOi3"
   },
   "outputs": [],
   "source": [
    "W = np.array((-24, 0.2, 0.2)).reshape((3,1))\n",
    "alpha = 0.001\n",
    "iterations = 50\n",
    "W, _ = lr_fit(X, W, Y, alpha, iterations)\n",
    "\n",
    "print(\"Values of the parameters W after 2000 iterations of gradient descent :\\n\"\n",
    "      + str(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHfZDebdgPC3"
   },
   "source": [
    "Expected values :\n",
    "\n",
    "[[-24.0001077 ]\n",
    "\n",
    " [  0.19653141]\n",
    " \n",
    " [  0.192503  ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LT_TLHOjtdNV"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = np.array(range(30, 100))\n",
    "y = (-1/W[2]) * (x * W[1] + W[0])\n",
    "\n",
    "plt.plot(x, y, c='red')\n",
    "plt.scatter(logistic_regression[logistic_regression[:, 2]==0, 0], logistic_regression[logistic_regression[:, 2]==0, 1], label='Admitted')\n",
    "plt.scatter(logistic_regression[logistic_regression[:, 2]==1, 0], logistic_regression[logistic_regression[:, 2]==1, 1], label='Not admitted', marker='v')\n",
    "\n",
    "\n",
    "plt.title('Scatter plot of training data', size='xx-large')\n",
    "plt.xlabel('Exam 1 score')\n",
    "plt.ylabel('Exam 2 score')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dSWscBuw290A"
   },
   "source": [
    "## 3. Linear regression with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wf4VD_BK3AtH"
   },
   "source": [
    "When you wan to apply linear regression you will noit use your implementation. People worked during long time to optimize code of linear regression and use more efficient version of gradient descent to optimize it. \n",
    "\n",
    "Sklearn is a python package which contain all the classic algorithm for data preprocessing, machine learning, and other tools. All algorithms are well optimized and easy to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iHAE7FNX4avf"
   },
   "source": [
    "Let's generate some linear-looking data to test sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4MrcCFHL4ZKL"
   },
   "outputs": [],
   "source": [
    "X_rand = 2 * np.random.rand(100, 1)\n",
    "Y_rand = 4 + 3 * X_rand + np.random.randn(100, 1)\n",
    "\n",
    "plt.scatter(X_rand, Y_rand)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ydYGZSB83sWJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# lin_reg contain linear regression initialized with default parameters \n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# .fit used to optimized the parameters \n",
    "lin_reg.fit(X_rand, Y_rand)\n",
    "\n",
    "# let's see our parameters\n",
    "print(\"w_0 : \"+str(np.squeeze(lin_reg.intercept_))+\"\\nw_1 : \" +str(np.squeeze(lin_reg.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cdrHiaQb51Py"
   },
   "outputs": [],
   "source": [
    "x_vizu = np.arange(0, 2.2, 0.1).reshape(22, 1)\n",
    "y_vizu = lin_reg.predict(x_vizu)\n",
    "\n",
    "plt.plot(x_vizu, y_vizu, c='red', label='predictions')\n",
    "plt.scatter(X_rand, Y_rand)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v2v68U-h3I4_"
   },
   "source": [
    "Now apply linear regression with sklearn at our previous problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D9r4gCfv6pJI"
   },
   "outputs": [],
   "source": [
    "m = len(univariate_regression[:, 0])\n",
    "\n",
    "# We construct the X dataset \n",
    "X = univariate_regression[:, 0].reshape(m, 1)\n",
    "\n",
    "\n",
    "\n",
    "# We construct the Y dataset\n",
    "Y = univariate_regression[:, 1].reshape(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2HeQkvjL6qLj"
   },
   "outputs": [],
   "source": [
    "### Your code start here ###\n",
    "\n",
    "### Your code end here ###\n",
    "\n",
    "print(\"w_0 : \"+str(np.squeeze(lin_reg.intercept_))+\"\\nw_1 : \"+str(np.squeeze(lin_reg.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "smJ8EKmujJhJ"
   },
   "source": [
    "Expected values : \n",
    "\n",
    "w_0 : -3.89\n",
    "\n",
    "w_1 : 1.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9CsQYIej63WE"
   },
   "outputs": [],
   "source": [
    "x_vizu = np.array(np.arange(4.5, 23)).reshape(19, 1)\n",
    "y_vizu = lin_reg.predict(x_vizu)\n",
    "\n",
    "plt.plot(x_vizu, y_vizu, c='red', label='predictions')\n",
    "plt.scatter(X, Y)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zoAWfjiy_NBP"
   },
   "source": [
    "## 4. Logistic regression with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JhqHTtg8jWs6"
   },
   "source": [
    "Let's generate some linear-looking data to test sklearn for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BvmsCBOO7QH8"
   },
   "outputs": [],
   "source": [
    "x_11 = 1 * np.random.randn(100) + 0\n",
    "x_21 = 3 * np.random.randn(100) + 2\n",
    "x_12 = 2 * np.random.randn(100) + 15\n",
    "x_22 = 6 * np.random.randn(100) + 10\n",
    "\n",
    "x_1 = np.concatenate((x_11, x_12))\n",
    "x_2 = np.concatenate((x_21, x_22))\n",
    "X = np.stack((x_1, x_2), axis=-1)\n",
    "\n",
    "Y = np.concatenate((np.repeat(0, 100), np.repeat(1, 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VYQF-85BBuI8"
   },
   "outputs": [],
   "source": [
    "plt.scatter(x_11, x_21, c = 'blue')\n",
    "plt.scatter(x_12, x_22, c = 'green')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ot6UrEzACUAy"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# log_reg contain logistic regression initialized with default parameters \n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# .fit used to optimized the parameters \n",
    "log_reg.fit(X, Y)\n",
    "\n",
    "w_0 = np.squeeze(log_reg.intercept_)\n",
    "w_1, w_2 = np.squeeze(log_reg.coef_)\n",
    "\n",
    "print(\"w_0 : \"+str(w_0)+\"\\nw_1 : \"+str(w_1)+\"\\nw_2 : \"+str(w_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiPSzlqeEZee"
   },
   "outputs": [],
   "source": [
    "x = np.array(range(2, 11))\n",
    "y = (-1/w_2) * (x * w_1 + w_0)\n",
    "\n",
    "plt.plot(x, y, c='red')\n",
    "plt.scatter(x_11, x_21, c = 'blue')\n",
    "plt.scatter(x_12, x_22, c = 'green')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zm8JRfRMH29T"
   },
   "outputs": [],
   "source": [
    "m, n = logistic_regression[:, :2].shape\n",
    "X = logistic_regression[:, :2]\n",
    "Y = logistic_regression[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOqiGvLL9loM"
   },
   "outputs": [],
   "source": [
    "### Your code start here ###\n",
    "\n",
    "### Your code end here ###\n",
    "\n",
    "w_0 = np.squeeze(log_reg.intercept_)\n",
    "w_1, w_2 = np.squeeze(log_reg.coef_)\n",
    "\n",
    "print(\"w_0 : \"+str(w_0)+\"\\nw_1 : \"+str(w_1)+\"\\nw_2 : \"+str(w_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3RC_Q_74uWjt"
   },
   "source": [
    "Expected values : \n",
    "\n",
    "w_0 : -3.89\n",
    "\n",
    "w_1 : 0.038\n",
    "\n",
    "w-2 : 0.031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gpmm0W4h99Is"
   },
   "outputs": [],
   "source": [
    "W = np.array((w_0, w_1, w_2))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = np.array(range(30, 80))\n",
    "y = (-1/W[2]) * (x * W[1] + W[0])\n",
    "\n",
    "plt.plot(x, y, c='red')\n",
    "plt.scatter(logistic_regression[logistic_regression[:, 2]==0, 0], logistic_regression[logistic_regression[:, 2]==0, 1], label='Admitted')\n",
    "plt.scatter(logistic_regression[logistic_regression[:, 2]==1, 0], logistic_regression[logistic_regression[:, 2]==1, 1], label='Not admitted', marker='v')\n",
    "\n",
    "\n",
    "plt.title('Scatter plot of training data', size='xx-large')\n",
    "plt.xlabel('Exam 1 score')\n",
    "plt.ylabel('Exam 2 score')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fyGt7dfikNWr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Lecture_1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
