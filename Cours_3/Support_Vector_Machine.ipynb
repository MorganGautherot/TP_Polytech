{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Support_Vector_Machine.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MorganGautherot/Machine_Learning_Courses/blob/master/Support_Vector_Machine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVRfyCS23vOZ"
      },
      "source": [
        "# Support Vector Machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUll1EwwpE5F"
      },
      "source": [
        "## 1 Linear Regression VS SVR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcTY-8NNpKS_"
      },
      "source": [
        "You will see the difference between linear regression and SVM regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9ujfr9upNi7"
      },
      "source": [
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
        "\n",
        "plt.scatter(X,y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILh57oPopZnI"
      },
      "source": [
        "Standardize data and train a regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5E6s0DDpVXC"
      },
      "source": [
        "### Your code start here ###\n",
        "\n",
        "# search in the sklearn documentation en use 'fit_transform'\n",
        "X_std = \n",
        "\n",
        "lin_reg =\n",
        "### Your code end here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex5o7XfRqtuB"
      },
      "source": [
        "x_lr = np.array(np.arange(-3, 3, 0.1)).reshape(60, 1)\n",
        "y_lr = lin_reg.predict(x_lr)\n",
        "plt.plot(x_lr, y_lr, color='red')\n",
        "\n",
        "plt.scatter(X,y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoBodOk1rsKi"
      },
      "source": [
        "Standardize data and train a Support Vector Machine regressor. Help you with the sklearn documentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0y-NeMLrqMQ"
      },
      "source": [
        "### Your code start here ###\n",
        "\n",
        "# search in the sklearn documentation en use 'fit_transform'\n",
        "X_std = \n",
        "\n",
        "svm = \n",
        "\n",
        "### Your code end here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57zXZMSWsAj6"
      },
      "source": [
        "x_lr = np.array(np.arange(-3, 3, 0.1)).reshape(60, 1)\n",
        "y_lr = svm.predict(x_lr)\n",
        "plt.plot(x_lr, y_lr, color='red')\n",
        "\n",
        "plt.scatter(X,y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey9YLPdmtQH2"
      },
      "source": [
        "As you can see SVM can better fit data than linear regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MWHs_dussi-"
      },
      "source": [
        "## 2 Logistic Regression VS SVC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYfWK1yDs8CP"
      },
      "source": [
        "You will see the difference between and SVM classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW6LdDBAs2G9"
      },
      "source": [
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=100, noise=0.1)\n",
        "\n",
        "plt.scatter(X[y==0][:,0], X[y==0][:,1], color='blue')\n",
        "plt.scatter(X[y==1][:,0], X[y==1][:,1], color='green')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH7NnjAotCml"
      },
      "source": [
        "Standardize your data en train a logsitic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g5PlpOUs3uT"
      },
      "source": [
        "### Your code start here ###\n",
        "\n",
        "# search in the sklearn documentation en use 'fit_transform'\n",
        "X_std = \n",
        "\n",
        "log_reg = \n",
        "\n",
        "### Your code end here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex4RZHabtOgq"
      },
      "source": [
        "W0 = log_reg.intercept_\n",
        "W = log_reg.coef_\n",
        "\n",
        "rl_x = np.array(range(-1, 3))\n",
        "rl_y = (-1/W[0, 1]) * (rl_x * W[0, 0] + W0[0])\n",
        "\n",
        "plt.plot(rl_x, rl_y, c='red')\n",
        "\n",
        "plt.scatter(X[y==0][:,0], X[y==0][:,1], color='blue')\n",
        "plt.scatter(X[y==1][:,0], X[y==1][:,1], color='green')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIZH0tQnu_kg"
      },
      "source": [
        "Standardize data and train a Support Vector Machine classifier Help you with the sklearn documentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlwkkrvRvJaO"
      },
      "source": [
        "### Your code start here ###\n",
        "\n",
        "# search in the sklearn documentation en use 'fit' then 'transform'\n",
        "scaler_fit =\n",
        "X_std = \n",
        "\n",
        "svm = \n",
        "\n",
        "### Your code end here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOynNkHuxHv1"
      },
      "source": [
        "def make_meshgrid(x, y, h=.02):\n",
        "    \"\"\"Create a mesh of points to plot in\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: data to base x-axis meshgrid on\n",
        "    y: data to base y-axis meshgrid on\n",
        "    h: stepsize for meshgrid, optional\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    xx, yy : ndarray\n",
        "    \"\"\"\n",
        "    x_min, x_max = x.min() - 1, x.max() + 1\n",
        "    y_min, y_max = y.min() - 1, y.max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    return xx, yy\n",
        "  \n",
        "def plot_contours(ax, clf, scaler, xx, yy, **params):\n",
        "    \"\"\"Plot the decision boundaries for a classifier.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ax: matplotlib axes object\n",
        "    clf: a classifier\n",
        "    xx: meshgrid ndarray\n",
        "    yy: meshgrid ndarray\n",
        "    params: dictionary of params to pass to contourf, optional\n",
        "    \"\"\"\n",
        "\n",
        "    Z = clf.predict(scaler.transform(np.c_[xx.ravel(), yy.ravel()]))\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    out = ax.contourf(xx, yy, Z, **params)\n",
        "    return out\n",
        "  \n",
        "X0, X1 = X[:, 0], X[:, 1]\n",
        "xx, yy = make_meshgrid(X0, X1)\n",
        "\n",
        "plot_contours(plt, svm, scaler_fit, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "plt.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n",
        "plt.xlabel('x2')\n",
        "plt.ylabel('x1')\n",
        "plt.xticks(())\n",
        "plt.yticks(())\n",
        "plt.title('SVM decision boundary')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2WbRedE4EgS"
      },
      "source": [
        "## 3 C hyperpramater in SVM regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZNmPMF54a4j"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DXib7s54ydw"
      },
      "source": [
        "To work we need data, run this piece of code and upload the dataset : univariate_regression.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7KZ-iGp4wUv"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36ik9dmw8cCV"
      },
      "source": [
        "svm_data_1 = np.genfromtxt('svm_data_1.txt', delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7C4XAyS8jUt"
      },
      "source": [
        "# We construct the X dataset \n",
        "X = svm_data_1[:, :2]\n",
        "\n",
        "# We construct the Y dataset\n",
        "y = svm_data_1[:, 2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihjFeA01fio8"
      },
      "source": [
        "First use what you saw on the Lecture 2 to standardize your data using sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ibXhRPjfsGN"
      },
      "source": [
        "### Your code start here ###\n",
        "\n",
        "\n",
        "# search in the sklearn documentation en use 'fit_transform'\n",
        "X_std = \n",
        "\n",
        "### Your code end here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wsd2bbw_lcl"
      },
      "source": [
        "We will begin by with a 2D example dataset which can be separated by a linear boundary. In this dataset, the positions of the positive examples (indicated by a triangle in green) and the negative examples (indicated bya circle in blue) suggest a natural separation indicated by the gap. However, notice that there is an outlier positive example a triangle in green on the far left at about (0.1, 4.1). As part of this exercise, you will also see how this outlier affects the SVM decision boundary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rriy73TS8w7h"
      },
      "source": [
        "plt.scatter(X[y==0, 0], X[y==0, 1], label='Admitted')\n",
        "plt.scatter(X[y==1, 0], X[y==1, 1], label='Not admitted', marker='v')\n",
        "plt.title('Example Dataset 1', size='xx-large')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N931RqeBAfd0"
      },
      "source": [
        "In this part of the exercise, you will try using different values of the C parameter with SVMs. Informally, the C parameter is a positive value that controls the penalty for misclassified training examples. A large C parameter tells the SVM to try to classify all the examples correctly. C plays a role similar to $\\frac{1}{\\lambda}$ , where $\\lambda$ is the regularization parameter that we were using previously for logistic regression.\n",
        "\n",
        "New cost function :\n",
        "\n",
        "$$J(W)=C[\\sum^{m}_{i}y^{(i)}cost_1(W^Tx{(i)})+(1-y^{(i)})cost_0(W^Tx^{(i)})]+\\frac{1}{2}\\sum^m_{i=1}w^2_j$$\n",
        "\n",
        "\n",
        "$$C=\\frac{1}{\\lambda}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jL3_AmgkLjs"
      },
      "source": [
        "Try different value of C and "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU80KKaB9nA7"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "### Your code start here ###\n",
        "svc = SVC(C=1, kernel='linear', gamma='scale')\n",
        "### Your code end here ###\n",
        "\n",
        "svc.fit(X, y)\n",
        "\n",
        "W0 = svc.intercept_\n",
        "W = svc.coef_\n",
        "\n",
        "rl_x = np.array(range(0, 5))\n",
        "rl_y = (-1/W[0, 1]) * (rl_x * W[0, 0] + W0[0])\n",
        "\n",
        "plt.plot(rl_x, rl_y, c='red')\n",
        "\n",
        "plt.scatter(X[y==0, 0], X[y==0, 1], label='Admitted')\n",
        "plt.scatter(X[y==1, 0], X[y==1, 1], label='Not admitted', marker='v')\n",
        "plt.title('SVM Decision Boundary in function of C', size='xx-large')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jEjwfP9kUUw"
      },
      "source": [
        "What do you think about the impact of C in this example ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2bemr1q0QDh"
      },
      "source": [
        "## 4 impact of C in SVM classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUnowXRileF9"
      },
      "source": [
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=100, noise=0.1)\n",
        "\n",
        "plt.scatter(X[y==0][:,0], X[y==0][:,1], color='blue')\n",
        "plt.scatter(X[y==1][:,0], X[y==1][:,1], color='green')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN9ZqFkz1Ofw"
      },
      "source": [
        "Play with C parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGm-EYUilIAc"
      },
      "source": [
        "std = StandardScaler()\n",
        "# search in the sklearn documentation en use 'fit' then 'transform'\n",
        "scaler_fit = std.fit(X)\n",
        "X_std = scaler_fit.transform(X)\n",
        "\n",
        "### Your code start here ###\n",
        "svm = SVC(C=1, kernel='rbf', gamma='scale')\n",
        "### Your code end here ###\n",
        "\n",
        "svm.fit(X_std, y)\n",
        "\n",
        "X0, X1 = X[:, 0], X[:, 1]\n",
        "xx, yy = make_meshgrid(X0, X1)\n",
        "  \n",
        "plot_contours(plt, svm, scaler_fit, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "plt.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n",
        "plt.xlabel('x2')\n",
        "plt.ylabel('x1')\n",
        "plt.xticks(())\n",
        "plt.yticks(())\n",
        "plt.title('SVM decision boundary')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdITIUqu1RJU"
      },
      "source": [
        "What do you think about C when it comes large ? And whne it comes small ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXQygU_H2KHm"
      },
      "source": [
        "Test different kernel (sigmoid, poly, linear, etc...)"
      ]
    }
  ]
}